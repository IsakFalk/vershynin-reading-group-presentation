\documentclass{article}
\usepackage{preamble}
\newcommand{\mc}[1]{\mathcal{#1}}

\title{Isotropy, Gaussian vector, spherical measure and concentration}
\author{Dimitri Meunier}
\date{13/04/2021}

\begin{document}
\maketitle

\begin{itemize}
\item Theorem 3.1.1: Concentration of the norm + deviation interpretation
\item Definition of two first moments for vectors
\item Isotropy and characterisation of isotropy
\item Exercise 3.3.1: the spherically distributed random variable is isotropic
\end{itemize}

\section{Random Vectors}

Let $(\Omega,\cA,\P)$ be a probability space and $(E,\cE)$ a measurable space.
Recall that a random variable is a measurable function $X:(\Omega,\cA) \to
(E,\cE)$. The distribution of $X$ denoted $\P_X$ is the probability measure on
$(E,\cE)$ defined for all $A \in \cA$ by $\P_X(A) = \P(X^{-1}(A))$ ($P_X$ is the
push-forward measure of $\P$ through $X$).

If $(E,\cE)$ is $(\R,\cB(\R))$, $X$ is called a \textbf{real random
variable} and if $(E,\cE)$ is $(\Rd,\cB(\Rd))$, $X$ is called a
\textbf{real random vector}. In the latter case, we denote by $X_i$,
$i=1,\ldots,d$ its coordinates, they are real random variables with distribution
$\P_{X_i} = \pi^i_{\#}\P_X$ where $\pi^i$ is the projection along axis $i$ and
$\#$ denotes the push-forward operator.

The Lebesgue measure on $(\Rd,\cB(\Rd))$ is denoted $\lambda_d$.

\subsection{Characteristic function}

In order to introduce Gaussian random vectors we first recall useful properties
of the characteric function.

\begin{definition}[Characteristic function]
  if $X$ is a real random vector, the characteristic function of $X$ is the function
  $\Phi_{X}: \mathbb{R}^{d} \longrightarrow \mathbb{C}$ defined by 
  $$
  \Phi_{X}(\xi)=E[\exp (i \xi \cdot X)] = \int e^{i \xi \cdot x} \P_{X}(d x), \quad \xi \in \mathbb{R}^{d}
  $$

  $\Phi_{X}$ is the Fourier transform of the distribution on $X$. The dominated
  convergence theorem shows that $\Phi_{X}$ is continuous (and bounded) on $\mathbb{R}^{d}$.
\end{definition}

\begin{theorem}
  The characteristic function of a real random vector $X$ characterised its
  distribution. In other words, the Fourier transform defined on the sapce of
  probability measures on $\Rd$ is injective. 
\end{theorem}

\begin{prop}
  If $X$ is a real random vector on $\Rd$, its coordinates are independant if and only if
  the characteristic function of $X$ is
  $$
  \Phi_{X}\left(\xi_{1}, \ldots, \xi_{d}\right)=\prod_{i=1}^{d} \Phi_{X_{i}}\left(\xi_{i}\right)
  $$
\end{prop}

\subsection{Gaussian vectors}

We first recall the density and the characteristic function of a univariate
Gaussian distribution.

\begin{definition} The standard normal (or Gaussian) distribution on $\R$ is the
  absolutely continuous measure (w.r.t to $\lambda_1$) with density, $$f(x)=(2
  \pi)^{-\frac{1}{2}} e^{-\frac{1}{2}x^{2}}.$$
  A random variable that follows this distribution is denoted $X \sim
  \cN_1(0,1)$. We say that $X \sim \cN_1(\mu,\sigma^2)$ if $X = \mu + \sigma Z$
  ($\sigma \geq 0$) where  $Z \sim \cN_1(0,1)$. If $\sigma >0$, the change of variable formula
  shows that, the density function of $X$ is then, $$f(x)=(2
  \pi\sigma^2)^{-\frac{1}{2}} e^{-\frac{1}{2\sigma^2}x^{2}}.$$
\end{definition}

\begin{prop} If $X \sim \cN_1(\mu,\sigma^2)$, then, 
  $$
  \Phi_{X}(\xi)=\exp \left(i\xi \mu -\frac{\sigma^{2} \xi^{2}}{2}\right), \quad \xi \in \mathbb{R}
  $$
\end{prop}

We are now ready to introduce the definition of a Gaussian random vector. 

\begin{definition}
  Let $X:(\Omega,\cA,\P) \to \Rd$ be a real random vector. $X$ is a \textbf{Gaussian
    vector} if for all $\theta \in \Rd$, $\langle X, \theta \rangle$ has a
  univariate normal distribution.
\end{definition}
From this definition we see that if $X$ is a vector of independent univariate
Gaussian variables, $X$ is a Gaussian vector. Indeed, for all $\theta \in \Rd$
and $\xi \in \R$, 
$$
\begin{aligned}
  \Phi_{<\theta, X>}(\xi ) &=E\left\{e^{i \xi \sum_{l=1}^{d} \theta_{l} X_{l}}\right\} \\
  &=\prod_{l=1}^{d} E\left\{e^{i \xi \theta_{l} X_{l}}\right\} \\
  &=\prod_{l=1}^{d} e^{\left(i \xi \theta_{l} \mu_{l}-\frac{1}{2} \xi^{2} \theta_{l}^{2}
      \sigma_{l}^{2}\right)} \qquad \text { if } X_{l} \sim
  \cN_{1}\left(\mu_{l}, \sigma_{l}^2 \right) \\
  &=e^{i \xi \sum_{l=1}^{d} \theta_{l} \mu_{l}-\frac{1}{2} \xi^{2} \sum_{l=1}^{d} \theta_{l}^{2} \sigma_{l}^{2}}
\end{aligned}
$$
Hence, by injectivity of the characteristic function,
$$
<\theta, X>\sim \cN_{1}\left(\sum_{l=1}^d \theta_{l} \mu_{l}, \sum_{l=1}^d \theta_{l}^{2} \sigma_{l}^{2}\right)
$$
Secondly, if $X$ is a Gaussian vector, for all $B \in \R^{r \times d}$ and $b
\in \R^r$, $Y = BX + b$ is also a Gaussian vector. Indeed for all $\theta \in
\R^r$, $\langle Y,\theta \rangle = \langle X,B^T\theta \rangle + \langle
\theta,b \rangle$ follows a univariate normal distribution.

\begin{theorem} A random vector $X: \Omega \rightarrow \mathbb{R}^{d}$ is
  Gaussian if and only if, there exists a vector $\mu \in \mathbb{R}^{d}$ and a
  symmetric matrice $K \in \R^{d \times d}$ such that, 
  
  \begin{equation} \label{eq:th}
    \Phi_{X}(\xi)=\exp \left(i \mu \cdot \xi-\frac{1}{2} \xi^t K \xi\right),
    \qquad \xi \in \Rd.
  \end{equation}
  
  Furthermore, $\mu$ and $K$ are the expectation and covariance of $X$. If $X$
  is a random variable that admits the characteristic function above we use the
  notation $X \sim \cN_d(\mu,K)$. 
\end{theorem}

\begin{proof}
  Let $X$ be a Gaussian vector, we first notice that for all $i=1,\ldots,d$,
  $\E[|X_i|^p] < \infty$ ($1\leq p < +\infty$). Indeed, $X_i = \langle X,e_i
  \rangle$ follows a univariate normal distribution. Therefore, the expectation
  $\mu:=\E[X]$ and covariance $K:=\E[(X-\mu)(X-\mu)^T]$ exist. 
  Let us fix $\theta \in \Rd$, since $Y:= \langle X,\theta \rangle \sim
  \cN_1(\mu^T\theta, \theta^TK\theta)$, we have, $$\Phi_{X}(\theta) = \Phi_Y(1)
  = e^{i\theta^t\mu - \theta^TK\theta/2}.$$

  For the converse, assume that $X$ is a random variable with a characteristic
  function as (\ref{eq:th}). Then for all $\theta \in \Rd$ and $\xi \in R$,
  $$\Phi_{\langle X,\theta \rangle}(\xi) = \Phi_X(\xi \theta)
  = e^{i \xi \mu^T\theta - \xi^2 \theta^TK\theta/2}.$$

  We recognise the characteristic function of a univariate Gaussian
  distribution. We conclude by injectivity of $\Phi$.

  \textcolor{blue}{+ Show $\mu=\E[X]$ and $K=\E[(X-\mu)(X-\mu)^T]$ (FURTHERMORE)}
\end{proof}

The theorem shows that a Gaussian vector is fully characterised by its two first
moments!

\begin{corollary}
  If $X$ is a Gaussian vector, its coordinates are independant if and only if
  the covariance is diagonal. 
\end{corollary}

\begin{proof}
  Indeed from the theroem, if $K$ is diagonal the characteristic function can be
  factorized in a product which characterised the independence. 
\end{proof}

\begin{definition}[Standard normal random vector]
  $X$ is called a \textbf{standard Gaussian} vector on $\Rd$ if its coordinates are i.i.d with distribution
  $\cN(0,1)$. By the last theorem, in that case $X \sim \cN_d(0,I_d)$.
\end{definition}

By independence of the coordinates we see that the
density function of $X \sim \cN_d(0,I_d)$ is

$$f(x)=(2 \pi)^{-\frac{d}{2}} e^{-\frac{1}{2}\|x\|_2^{2}}, \qquad x \in \Rd.$$

\begin{theorem}
  Let $X$ be a Gaussian vector with mean $\mu$ and covariance $K$ (i.e. $X \sim \cN{\mu,K}$), then $X =
  K^{1/2}Z + \mu$, where $Z \sim \cN_d(0,I_d)$ and the equality holds in
  distribution. 
\end{theorem}

\begin{proof}
  $K$ is a covariance matrix which is a semi-definite positive symmetric matrix,
  hence there exists an orthogonal matrix $U$ and a diagonal matrix $D$ (with nonnegative diagonal
  elements) such that $K = UDU^T$. Recall that the square root of a
  semi-definite positive symmetric matrix is defined as $K^{1/2}:= UD^{1/2}U^T$ (the
  definition makes sense since $U^T = U^{-1}$, $K^{1/2}K^{1/2} = K$).\\
  $Z \sim \cN_d(0,I_d)$ is a Gaussian vector and we have seen that any affine transormation of a
  Gaussian vector is a Gaussian vector, therefore $Y:= K^{1/2}Z + \mu$ is a
  Gaussian vector. As mentioned previously, a Gaussian vector is characterised
  by its two first moments and $\E[Y] =K^{1/2}\E[Z]+\mu = \mu$ and $\mathbb{V}[Y] =
  K^{1/2}\mathbb{V}[Z]K^{1/2} = K$, Q.E.D.
\end{proof}

\begin{prop}
  If $X \sim \cN(\mu,K)$, $X$ admits a density if and only if $K$ is
  invertible and it that case, its density function is,
  $$f(x)=|2 \pi K|^{-\frac{1}{2}} e^{-\frac{1}{2}\|x - \mu\|_{K^{-1}}^{2}}$$
\end{prop}

$\|.\|_A$ is the Mahalanobis distance (which is a norm for  definite positive matrices).

\begin{proof}
  We have seen that $X = K^{1/2}Z + \mu$, where $Z \sim \cN_d(0,I_d)$ and the equality holds in
  distribution. The result follows from change of variable on the density of the
  standard Gaussian density through the $C^1-$diffeomorphism $\phi:
  x \in \Rd \to K^{-1/2}(x - \mu)$.
\end{proof}

\section{Spherical Measure and Normal distribution}

In this section the sphere is denoted $S^{d-1} = \{x \mid \|x\|=1\}$ and the
unit ball $B^d = \{x \mid \|x\| \leq 1\}$. The set of vectorial isometries on
$\Rd$ is $\{\phi:\Rd \to \Rd \text{ linear } \mid \|\phi(x)\|=\|x\| \forall x
\in \Rd \}$. The set of associated matrices is the orthogonal group $O(d) = \{A
\in \R^{d \times d} | A^TA = I_d\}$. 

\textbf{Goals:}
\begin{itemize}
\item define a measure $\omega_d$ on
  $(S^{d-1},\cB(S^{d-1}))$ that is invariant to isometries, in order to have a
  canonical measurable space $(S^{d-1},\cB(S^{d-1}), \omega_d)$ on the sphere.
\item introduce the change of variable in polar coordinates
\end{itemize}

The Lebesgue measure $\lambda_d$ on $\Rd$ is the unique (up to constants)
translation-invariant measure on $\Rd$. Similarly $\omega_d$ is the unique (up to
constants) measure on $S^{d-1}$ invariant to isometries.

\begin{definition}
  If $A \in \mathcal{B}\left(S^{d-1}\right)$, we define the \emph{wedge} $\Gamma(A)$ the Borel set
  of $\mathbb{R}^{d}$ defined by
  $$
  \Gamma(A)=\{r x ; r \in[0,1] \text { and } x \in A\}
  $$
  For all $A \in \mathcal{B}\left(S^{d-1}\right)$, the measure,
  $$
  \omega_{d}(A)=d \lambda_{d}(\Gamma(A))
  $$
  is called the \textbf{spherical measure}. 
\end{definition}

\begin{theorem}
  $\omega_d$ is invariant to isometries and for any measurable function $f: \Rd \to \R_+$, $$\boxed{\int_{\R^{d}} f(x) d x=\int_{S^{d-1}}\left(\int_{0}^{\infty} f(r \gamma) r^{d-1} d r\right) d \omega_d(\gamma)}$$
\end{theorem}

+ integrable setting? 

\begin{prop}
  The volume of the d-dimensional ball $B^d = \{x \mid \|x\| \leq 1\} $ is $\frac{\pi^{\frac{d}{2}}}{\Gamma\left(\frac{d}{2}+1\right)}$.
\end{prop}

\begin{proof}
  It is an application of the Fubini theorem.
\end{proof}
Therefore, $\omega_d(S^{d-1}) = d\lambda_d(B^d) = d\frac{\pi^{d /
    2}}{\Gamma\left(\frac{d}{2}+1\right)} = \frac{2 \pi^{d /
    2}}{\Gamma\left(\frac{d}{2}\right)} $, and we define the \textbf{uniform
  probability distribution on the sphere} as

\begin{equation}
  \sigma_{d}(A):=\frac{\Gamma\left(\frac{d}{2}+1\right)}{\pi^{d / 2}} \lambda_{d}(\{r x: 0 \leq r \leq 1, x \in A\}).
\end{equation}

\paragraph{Remark.} If $f$ is radial, i.e. $f: \Rd\to \R_+$ and there exists $g: \R
\to \R_+$ such that $f(x) = g(\|x\|)$ for all $x \in \Rd$ then the change of
variable formula leads to,

\begin{equation}
  \int_{\R^{d}} f(x) d x= \omega_d(S^{d-1})\int_{0}^{\infty} g(r)r^{d-1} d r
\end{equation}

\begin{prop}
  The measure $\sigma_{d}$ is the unique probability measure on the sphere
  $S^{d-1}$ invariant to the action of vectorial isometries.
\end{prop}

+ Link to the Haar measure

\begin{prop}[Exercise 3.3.7] Let us write $X \sim N_d\left(0, I_{d}\right)$ in
  polar  form as
  $$
  X=R \theta
  $$
  where $R=\|X\|_{2}$ is the length and $\theta=X /\|X\|_{2}$ is the direction
  of $X$. Prove the following:

  \begin{enumerate}
  \item the length $R$ and direction $\theta$ are independent random variables
  \item the direction $\theta$ is uniformly distributed on the unit sphere
    $S^{d-1}$
  \item (Bonus) the length $R$ follows a generalized gamma distribution
  \end{enumerate}
\end{prop}

\begin{proof}
  We note $\rho$ the density of $X \sim \cN_d(0,I_d)$. We want to compute the
  distribution of $R$ and $\theta$ where $(R,\theta) = (\|X\|_2,X/\|X\|_2)$ is a random
  vector with values in $\R \times S^{d-1}$. \\ For all measurable function
  $h:\R \times S^{d-1} \to \R$ positive or bounded, 

\begin{equation}
\begin{aligned}
  \E[h(R,\theta)] &= \int_{\R^{d}} h(\|x\|,x/\|x\|)\rho(x) d x \\  &= \int_{S^{d-1}}\left(\int_{0}^{\infty} h(r,\theta) \rho(r\theta) r^{d-1} d r\right) d \omega_d(\theta) \\
  &= \int_{S^{d-1}}\left(\int_{\R} h(r,\theta) \underbrace{\frac{e^{-r^2/2}}{(2\pi)^{d/2}} r^{d-1}1_{r \geq 0}}_{=: g(r,\theta)} d r\right) d \omega_d(\theta)
\end{aligned}
\end{equation}

$g$ is the density of $(R,\theta)$, we notice that $g$ is constant in with
respect to $\theta$, it implies both that $R$ and $\theta$ are independent and
that $\theta$ is uniformely distributed on the sphere.

As a sanity check we can explicitely compute the constants. The part of
the density that depends on $r$ is $e^{-r^2/2} r^{d-1}1_{r \geq 0}$, it is the un-normalized
density function of a \textbf{generalized gamma distribution}. Therefore, the
density function of $R$ is\footnote{without knowing the generalized gamma density function, the
  normalisation constant can be obtained from the gamma density
  function by applying the change of variable $\phi(x) = \sqrt{x}$},

$$f_{\gamma}(r) = e^{-(r/\sqrt{2})^2} r^{d-1} \frac{2}{\Gamma(d/2)2^{d/2}}1_{r\
  \geq 0}$$
Thus,

$$ g(r,\theta) = f_{\gamma}(r) \times \frac{\Gamma(d/2)2^{d/2}}{2(2\pi)^{d/2}} = f_{\gamma}(r) \times \frac{\Gamma(d/2)}{2\pi^{d/2}} = f_{\gamma}(r) \times \omega_d(S^{d-1})^{-1}$$

\end{proof}

\subsection{Gaussian concentration}

Applying theorem 1 (Isak) to, $X \sim \cN_d(0,I_d)$ we get, CONSTANTS (depends
on $d$ ??)

\begin{equation}
  \mathbb{P}\left\{\left|\|X\|_{2}-\sqrt{d}\right| \geq t\right\} \leq 2 \exp \left(-c t^{2}\right) \quad \text { for all } t \geq 0
\end{equation}

Using the notations of the last section, it says that $R \approx \sqrt{d}$ with
high probability. Morevover, $X = RS \approx \sqrt{n}S \sim
Unif(\sqrt{n}S^{d-1})$.

Say more?

\section{Sub-Gaussian vectors}



\end{document}
