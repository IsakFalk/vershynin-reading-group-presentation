\documentclass{article}
\usepackage{preamble}
\newcommand{\mc}[1]{\mathcal{#1}}

\title{Isotropy, Gaussian vector, spherical measure and concentration}
\author{Dimitri Meunier}
\date{13/04/2021}

\begin{document}
\maketitle

\begin{itemize}
\item Theorem 3.1.1: Concentration of the norm + deviation interpretation
\item Definition of two first moments for vectors
\item Isotropy and characterisation of isotropy
\item Exercise 3.3.1: the spherically distributed random variable is isotropic
\end{itemize}

\section{Random Vectors}

Let $(\Omega,\cA,\P)$ be a probability space and $(E,\cE)$ a measurable space.
Recall that a random variable is a measurable function $X:(\Omega,\cA) \to
(E,\cE)$. The distribution of $X$ denoted $\P_X$ is the probability measure on
$(E,\cE)$ defined for all $A \in \cA$ by $\P_X(A) = \P(X^{-1}(A))$ ($P_X$ is the
push-forward measure of $\P$ through $X$).

If $(E,\cE)$ is $(\R,\cB(\R))$, $X$ is a \textbf{real} random
variable and if $(E,\cE)$ is $(\Rd,\cB(\Rd))$, $X$ is a
\textbf{real random vector}. In the latter case, we denote by $X_i$,
$i=1,\ldots,d$ its coordinates, they are real random variables with distribution
$\P_{X_i} = \pi^i_{\#}\P_X$ where $\pi^i$ is the projection along axis $i$ and
$\#$ denotes the push-forward operator.

+ Lp spaces, and Lebesgue measure

\section{Gaussian vectors}

Recall univariate density and univariate characteristic function. Recall
characteristic function for vector and characterisaion. Independence. 

\begin{definition}
  Let $X:(\Omega,\cA,\P) \to \Rd$ be a random vector. $X$ is a \textbf{Gaussian
    vector} if for all $\theta \in \Rd$, $\langle X, \theta \rangle$ has a
  univariate normal distribution.
\end{definition}

From this definition we see that if $X$ is a vector of independent univariate
Gaussian variables, $X$ is a Gaussian vector (use the characteristic function).
Secondly, if $X$ is a Gaussian vector, for all $B \in \R^{r \times d}$ and $b
\in R^r$, $Y = BX + b$ is also a Gaussian vector. Indeed for all $\theta \in
\R^r$, $\langle Y,\theta \rangle = \langle X,B^T\theta \rangle + \langle
\theta,b \rangle$ follows a univariate normal distribusion.

\begin{theorem} A random vector $X: \Omega \rightarrow \mathbb{R}^{d}$ is
  Gaussian if and only if, there exists a vector $\mu \in \mathbb{R}^{d}$ and a
  symmetric matrice $K \in \R^{d \times d}$ such that, 
  $$
  \Phi_{X}(\theta)=\exp \left(-i \mu \cdot \theta-\frac{1}{2} \theta^t K \theta\right)
  $$
  Furthermore, $\mu$ and $K$ are the expectation and covariance of $X$. + name distribution
\end{theorem}

\begin{proof}
  Let $X$ be a Gaussian vector, we first notice that for all $i=1\ldots,d$,
  $X_i \in \cL^p(\R)$ ($1\leq p < +\infty$). Indeed $X_i = \langle X,e_i
  \rangle$ follows a univariate normal distribution. Therefore the expectation
  $\mu:=\E[X]$ and covariance $K:=\E[(X-m)(X-m)^T]$ exist. 
  Let us fix $\theta \in \Rd$, we know that $Y:= \langle X,\theta \rangle \sim
  \cN(\mu^T\theta, \theta^TK\theta)$. Therefore, $$\Phi_{X}(\theta) = \Phi_Y(1)
  = e^{i\theta^t\mu - \theta^TK\theta/2}.$$
\end{proof}

\begin{definition}[Standard normal random vector]
  $X$ is called a \textbf{standard Gaussian} vector on $\Rd$ if its coordinates are i.i.d with distribution
  $\cN(0,1)$. We denote the distribution of $X$, $\cN_d(0,I_d)$. + moments 1 and
  2 + they characterise the law. 
\end{definition}

Recall that the density function of the univariate standard normal distribution on
$\R$ is $f(x)=(2 \pi)^{-\frac{1}{2}} e^{-\frac{1}{2}x^{2}}$. Therefore, the
density function of $X \sim \cN_d(0,I_d)$ is, for all $x \in \Rd$,

$$f(x)=(2 \pi)^{-\frac{d}{2}} e^{-\frac{1}{2}\|x\|_2^{2}}$$

\begin{prop}
  If $X$ is a Gaussian vector, its coordinates are independant if and only if
  the covariance if diagonal. 
\end{prop}

\begin{proof}
  Indeed from the theroem, if $K$ is diagonal the characteristic function can be
  factorized which is a characterisation of independence. 
\end{proof}

\begin{theorem}
  Let $X$ be a Gaussian vector with mean $\mu$ and covariance $K$, then $X =
  K^{1/2}Z + \mu$. Where $Z \sim \cN_d(0,I_d)$ and the equality holds in
  distribution. 
\end{theorem}

\begin{proof}
  $K$ is a covariance which is a positive symmetric matrice, hence there exists an
  orthogonal matrice $U$ and a diagonal matrice $D$ (with nonnegative diagonal
  elements) such that $K = UDU^T$. Recall that $K^{1/2}:= UD^{1/2}U^T$, the
  definition makes sense since $U^T = U^{-1}$, $K^{1/2}K^{1/2} = K$.\\
  $Z$ is a Gaussian vector and we have seen that any affine transormation of a
  Gaussian vector is a Gaussian vector therefore $Y:= K^{1/2}Z + \mu$ is a
  Gaussian vector. Since $\E[Y] =K^{1/2}\E[Z]+\mu = \mu$ and $\mathbb{V}[Y] =
  K^{1/2}\mathbb{V}[Z])K^{1/2} = K$. 
\end{proof}

\paragraph{Remark.} Isak: define moments of vector and Lp space for vectors
(equivalence with each coordinates in standard Lp). Cartesian norm.

\begin{prop}
  If $X$ is a Gaussian vector with mean $\mu$ and variance $K$ we use the
  notation $X \sim \cN(\mu,K)$. $X$ admits a density if and only if $K$ is
  invertible. Its density is,
  $$f(x)=|2 \pi K|^{-\frac{1}{2}} e^{-\frac{1}{2}\|x - \mu\|_{K^{-1}}^{2}}$$
\end{prop}

+ Mahanobis distance.

\begin{proof}
  Apply a change of variable to the density of the standard Gaussian density. 
\end{proof}

\section{Spherical Measure and Normal distribution}

+ define group $O(n)$.

$S^{d-1} = \{x \mid \|x\|=1\}$. \textbf{Goals:}
\begin{itemize}
\item define a measure $\omega_d$ on
  $(S^{d-1},\cB(S^{d-1}))$ that is invariant to rotations in order to have a
  canonical ``Lebesgue'' space $(S^{d-1},\cB(S^{d-1}), \omega_d)$ on the sphere.
\item introduce the change of variable in polar coordinates
\end{itemize}

Similarly to the Lebesgue measure on $\Rd$ being the unique (up to constants)
translation-invariant measure on $\Rd$, $\omega_d$ is the unique (up to
constants) measure on $S^{d-1}$ rotation-invariant.

\begin{definition}
  If $A \in \mathcal{B}\left(S^{d-1}\right)$, we define $\Gamma(A)$ the Borel set
  of $\mathbb{R}^{d}$ defined by
  $$
  \Gamma(A)=\{r x ; r \in[0,1] \text { and } x \in A\}
  $$
  For all $A \in \mathcal{B}\left(S^{d-1}\right)$, the measure,
  $$
  \omega_{d}(A)=d \lambda_{d}(\Gamma(A))
  $$
  is called the spherical measure. 
\end{definition}

\begin{theorem}
  $\omega_d$ is invariant to isometries and for any measurable function $f: \Rd \to \R_+$, $$\boxed{\int_{\R^{d}} f(x) d x=\int_{S^{d-1}}\left(\int_{0}^{\infty} f(r \gamma) r^{d-1} d r\right) d \omega_d(\gamma)}$$
\end{theorem}

\begin{prop}
  The volume of the d-dimensional ball $B^d = \{x \mid \|x\| \leq 1\} $ is $\frac{\pi^{\frac{d}{2}}}{\Gamma\left(\frac{d}{2}+1\right)}$
\end{prop}

\begin{proof}
  It is an application of Fubini theorem.
\end{proof}
Therefore, $\omega_d(S^{d-1}) = d\lambda_d(B^d) = d\frac{\pi^{d /
    2}}{\Gamma\left(\frac{d}{2}+1\right)} = \frac{2 \pi^{d /
    2}}{\Gamma\left(\frac{d}{2}\right)} $, and the \textbf{uniform probability
  on the sphere} is,

\begin{equation}
  \sigma_{d}(A):=\frac{\Gamma\left(\frac{d}{2}+1\right)}{\pi^{d / 2}} \lambda_{d}(\{r x: 0 \leq r \leq 1, x \in A\})
\end{equation}

\paragraph{Remark.} If $f$ is radial, i.e. $f: \Rd\to \R_+$ and there exists $g: \R
\to \R_+$ such that $f(x) = g(\|x\|)$ for all $x \in \Rd$ then the change of
variable formula leads to,

\begin{equation}
  \int_{\R^{d}} f(x) d x= \omega_d(S^{d-1})\int_{0}^{\infty} g(r)r^{d-1} d r
\end{equation}

\begin{prop}
  The measure $\sigma_{d}$ is the unique probaility measure on the sphere
  $S^{d-1}$ invariant to the action of vectorial isometries.
\end{prop}

+ Link to the Haar measure

\begin{prop}[Exercise
  3.3.7 Vershynin: sampling on the unit sphere with a Normal distribution] Let
  us write $X \sim N_d\left(0, I_{d}\right)$ in polar form as
  $$
  X=r \theta
  $$
  where $R=\|X\|_{2}$ is the length and $S=X /\|X\|_{2}$ is the direction
  of $X$. Prove the following:

  \begin{enumerate}
  \item the length $R$ and direction $S$ are independent random variables
  \item the direction $S$ is uniformly distributed on the unit sphere
    $S^{d-1}$
  \item (Bonus) the length $R$ follows a generalized gamma distribution
  \end{enumerate}
\end{prop}

\begin{proof}
  We note $\rho$ the density of $X \sim \cN_d(0,I_d)$. We want to compute the
  distribution of $R$ and $S$ where $(R,S) = (\|X\|_2,X/\|X\|_2)$ is a random
  vector with values in $\R \times S^{d-1}$. \\ For all measurable function
  $h:\R \times S^{d-1} \to \R$ positive or bounded, 

\begin{equation}
\begin{aligned}
  \E[h(R,S)] &= \int_{\R^{d}} h(x/\|x\|,\|x\|)\rho(x) d x \\  &= \int_{S^{d-1}}\left(\int_{0}^{\infty} h(\gamma,r) \rho(r\gamma) r^{d-1} d r\right) d \omega_d(\gamma) \\
  &= \int_{S^{d-1}}\left(\int_{\R} h(\gamma,r) \underbrace{\frac{e^{-r^2/2}}{(2\pi)^{d/2}} r^{d-1}1_{r \geq 0}}_{=: g(\gamma.r)} d r\right) d \omega_d(\gamma)
\end{aligned}
\end{equation}

$g$ is the density of $(R,S)$, we notice that $g(\gamma,r)$ is separable which
implies the independence. Secondly $g$ is constant in $\gamma$ which implies
that $S$ is uniformely distributed on the sphere.

As a sanity check we can explicitely compute the constants (bonus). The part of
the density that depends on $r$ is $e^{-r^2/2} r^{d-1}1_{r \geq 0}$, it is the un-normalized
density of a \textbf{generalized gamma distribution} $\Gamma(d,\sqrt{2},2)$.
Therefore, $R$ follows a $\Gamma(d,\sqrt{2},2)$ distribution and the normalized
density function\footnote{without knowing the generalized gamma density function, the
  normalisation constant can be obtained from the gamma density
  function by applying the change of variable $\phi(x) = \sqrt{x}$} is,

$$f_{\gamma}(r) = e^{-(r/\sqrt{2})^2} r^{d-1} \frac{2}{\Gamma(d/2)2^{d/2}}1_{r\
  \geq 0}$$
Thus,

$$ g(\gamma,r) = f_{\gamma}(r) \times \frac{\Gamma(d/2)2^{d/2}}{2(2\pi)^{d/2}} = f_{\gamma}(r) \times \frac{\Gamma(d/2)}{2\pi^{d/2}} = f_{\gamma}(r) \times \omega_d(S^{d-1})^{-1}$$

\end{proof}

\subsection{Gaussian concentration}

Applying theorem 1 (Isak) to, $X \sim \cN_d(0,I_d)$ we get, CONSTANTS (depends
on $d$ ??)

\begin{equation}
  \mathbb{P}\left\{\left|\|X\|_{2}-\sqrt{d}\right| \geq t\right\} \leq 2 \exp \left(-c t^{2}\right) \quad \text { for all } t \geq 0
\end{equation}

Using the notations of the last section, it says that $R \approx \sqrt{d}$ with
high probability. Morevover, $X = RS \approx \sqrt{n}S \sim
Unif(\sqrt{n}S^{d-1})$.

Say more?

\section{Sub-Gaussian vectors}



\end{document}
